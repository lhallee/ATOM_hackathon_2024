{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization\n",
    "Hyperparameters dictate the parameters of the training process and the architecture of the model itself. For example, the \n",
    "number of random trees is a hyperparameter for a **random forest**. In contrast, a learned parameter for a **random forest** is the set of features that is contained in a single node (in a single tree) and the cutoff values for each of those features that determines how the data is split at that node. A full discussion of hyperparameter optimization can be found on **[Wikipedia](https://en.wikipedia.org/wiki/Hyperparameter_optimization)**.\n",
    "\n",
    "The choice of hyperparameters strongly influences model performance,\n",
    "so it is important to be able to optimize them as well. **[AMPL](https://github.com/ATOMScience-org/AMPL)**\n",
    "offers a variety of hyperparameter optimization methods including\n",
    "random sampling, grid search, and Bayesian optimization. Please refer to the parameter documentation \n",
    "**[page](https://github.com/ATOMScience-org/AMPL#hyperparameter-optimization)** for further information.\n",
    "\n",
    "In this tutorial we demonstrate the following:\n",
    "- Build a parameter dictionary to perform a hyperparameter search for a **random forest** using Bayesian optimization.\n",
    "- Perform the optimization process.\n",
    "- Review the results\n",
    "\n",
    "We will use these **[AMPL](https://github.com/ATOMScience-org/AMPL)** functions:\n",
    "- [parse_params](https://ampl.readthedocs.io/en/latest/utils.html#utils.hyperparam_search_wrapper.parse_params)\n",
    "- [build_search](https://ampl.readthedocs.io/en/latest/utils.html#utils.hyperparam_search_wrapper.build_search)\n",
    "- [run_search](https://ampl.readthedocs.io/en/latest/utils.html#utils.hyperparam_search_wrapper.HyperOptSearch.run_search)\n",
    "- [get_filesystem_perf_results](https://ampl.readthedocs.io/en/latest/pipeline.html#pipeline.compare_models.get_filesystem_perf_results)\n",
    "\n",
    "The first three functions in the above list come from the `hyperparameter_search_wrapper` module. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up Directories\n",
    "\n",
    "Here we set up a few important variables corresponding to required directories and specific features for the **hyperparameter optimization (HPO)** process. Then, we ensure that the directories are created before saving models into them.\n",
    "\n",
    "|Variable|Description|\n",
    "|---|---|\n",
    "|`dataset_key`|The relative path to the dataset you want to use for HPO|\n",
    "|`descriptor_type`|The type of features you want to use during HPO|\n",
    "|`model_dir`|The directory where you want to save all of the models|\n",
    "|`best_model_dir`|For Bayesian optimization, the winning model is saved in this separate folder|\n",
    "|`split_uuid`|The presaved split uuid from **Tutorial 2, \"Splitting Datasets for Validation and Testing\"**|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "\n",
    "import os\n",
    "\n",
    "dataset_key='dataset/final_data_curated.csv'\n",
    "descriptor_type = 'rdkit_raw'\n",
    "model_dir = 'dataset/final_models'\n",
    "best_model_dir = 'dataset/final_models/best_models'\n",
    "split_uuid = \"f346a459-b654-4e1a-9bb4-8c4207a19b1f\"\n",
    "\n",
    "\n",
    "if not os.path.exists(f'./{best_model_dir}'):\n",
    "    os.mkdir(f'./{best_model_dir}')\n",
    "    \n",
    "if not os.path.exists(f'./{model_dir}'):\n",
    "    os.mkdir(f'./{model_dir}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run a hyperparameter search, we first create a parameter dictionary with parameter settings that will be common to all models, along with some special parameters that control the search and indicate which parameters will be varied and how. The table below describes the special parameter settings for our random forest search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Dictionary Settings\n",
    "\n",
    "|Parameter|Description|\n",
    "|---|---|\n",
    "|`'hyperparam':'True'`|This setting indicates that we are performing a hyperparameter search instead of just training one model.|\n",
    "|`'previously_featurized':'True'`|This tells **[AMPL](https://github.com/ATOMScience-org/AMPL)** to search for previously generated features in `../dataset/scaled_descriptors` instead of regenerating them on the fly.|\n",
    "|`'search_type':'hyperopt'`|This specifies the hyperparameter search method. Other options include `grid`, `random`, and `geometric`. Specifications for each hyperparameter search method is different, please refer to the full documentation. Here we are using the Bayesian optimization method.|\n",
    "|`'model_type':'RF\\|10'`|This means **[AMPL](https://github.com/ATOMScience-org/AMPL)** will try 10 times to find the best set of hyperparameters using **random forests**. In practice, this parameter could be set to 100 or more.|\n",
    "|`'rfe':'uniformint\\|8,512'`|The Bayesian optimizer will uniformly search between 8 and 512 for the best number of random forest estimators. Similarly `rfd` stands for **random forest depth** and `rff` stands for **random forest features**.|\n",
    "|`result_dir`|Now expects two parameters. The first directory will contain the best trained models while the second directory will contain all models trained in the search.|\n",
    "\n",
    "Regression models are optimized to maximize the $R^2$ and\n",
    "classification models are optimized using area under the \n",
    "receiver operating characteristic curve.\n",
    "A full list of parameters can be found on our\n",
    "**[github](https://github.com/ATOMScience-org/AMPL/blob/master/atomsci/ddm/docs/PARAMETERS.md)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"hyperparam\": \"True\",\n",
    "    \"prediction_type\": \"regression\",\n",
    "\n",
    "    \"dataset_key\": dataset_key,\n",
    "    \"id_col\": \"compound_id\",\n",
    "    \"smiles_col\": \"base_rdkit_smiles\",\n",
    "    \"response_cols\": \"Y\",\n",
    "\n",
    "    \"splitter\":\"scaffold\",\n",
    "    \"split_uuid\": split_uuid,\n",
    "    \"previously_split\": \"True\",\n",
    "\n",
    "    \"featurizer\": \"computed_descriptors\",\n",
    "    \"descriptor_type\" : descriptor_type,\n",
    "    \"transformers\": \"True\",\n",
    "\n",
    "    \"search_type\": \"hyperopt\",\n",
    "    \"model_type\": \"RF|10\",\n",
    "    \"rfe\": \"uniformint|8,512\",\n",
    "    \"rfd\": \"uniformint|6,32\",\n",
    "    \"rff\": \"uniformint|8,200\",\n",
    "\n",
    "    \"result_dir\": f\"./{best_model_dir},./{model_dir}\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Hyperparameter Search\n",
    "In **Tutorial 3, \"Train a Simple Regression Model\"**, we directly imported the `parameter_parser` and `model_pipeline` objects to parse the `config` dict and train a single model. Here, we use `hyperparameter_search_wrapper` to handle many models for us. First we build the search by creating a list of parameters to use, and then we run the search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_performance|train_r2|train_rms|valid_r2|valid_rms|test_r2|test_rms|model_params|model\n",
      "\n",
      "xgb_gamma: 0.16049048354303894, xgb_learning_rate: 0.7937793192040994, xgb_max_depth: 6, xgb_colsample_bytree: 1.0, xgb_subsample: 1.0, xgb_n_estimators: 100, xgb_min_child_weight: 1.0\n",
      "xgboost model with computed_descriptors and rdkit_raw                                                    \n",
      "  0%|                                                             | 0/10 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-19 14:44:25,438 Previous dataset split restored\n",
      "/opt/atomsci-env/lib/python3.8/site-packages/xgboost/core.py:158: UserWarning: [14:44:25] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "\n",
      "/opt/atomsci-env/lib/python3.8/site-packages/xgboost/core.py:158: UserWarning: [14:44:25] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"n_gpus\", \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_performance|0.917|0.271|0.091|1.067|0.052|1.147|0.16049048354303894_0.7937793192040994_6_1.0_1.0_100_1.0|./dataset/final_models/final_data_curated_model_35f616fd-0468-4b0e-87a2-5b55960da2d4.tar.gz\n",
      "\n",
      "xgb_gamma: 0.1603923077618593, xgb_learning_rate: 5.428415252269048, xgb_max_depth: 6, xgb_colsample_bytree: 1.0, xgb_subsample: 1.0, xgb_n_estimators: 100, xgb_min_child_weight: 1.0\n",
      "xgboost model with computed_descriptors and rdkit_raw                                                    \n",
      " 10%|███▌                               | 1/10 [00:00<00:05,  1.75trial/s, best loss: 0.9092469922305699]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-19 14:44:26,010 Previous dataset split restored\n",
      "/opt/atomsci-env/lib/python3.8/site-packages/xgboost/core.py:158: UserWarning: [14:44:26] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"n_gpus\", \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_performance|0.000|100.000|0.000|100.000|0.000|100.000|0.1603923077618593_5.428415252269048_6_1.0_1.0_100_1.0|./dataset/final_models/final_data_curated_model_27514994-b5aa-48b7-9b28-546e255b3cf7.tar.gz\n",
      "\n",
      "xgb_gamma: 0.09115948049300145, xgb_learning_rate: 0.15604420029845428, xgb_max_depth: 6, xgb_colsample_bytree: 1.0, xgb_subsample: 1.0, xgb_n_estimators: 100, xgb_min_child_weight: 1.0\n",
      "xgboost model with computed_descriptors and rdkit_raw                                                    \n",
      " 20%|███████                            | 2/10 [00:01<00:04,  1.93trial/s, best loss: 0.9092469922305699]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-19 14:44:26,488 Previous dataset split restored\n",
      "/opt/atomsci-env/lib/python3.8/site-packages/xgboost/core.py:158: UserWarning: [14:44:26] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"n_gpus\", \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_performance|0.823|0.396|0.347|0.905|0.289|0.993|0.09115948049300145_0.15604420029845428_6_1.0_1.0_100_1.0|./dataset/final_models/final_data_curated_model_b6763e7f-d080-4cb1-863e-f9f177fc7433.tar.gz\n",
      "\n",
      "xgb_gamma: 0.13276616347209194, xgb_learning_rate: 1.5287856539810256, xgb_max_depth: 6, xgb_colsample_bytree: 1.0, xgb_subsample: 1.0, xgb_n_estimators: 100, xgb_min_child_weight: 1.0\n",
      "xgboost model with computed_descriptors and rdkit_raw                                                    \n",
      " 30%|██████████▌                        | 3/10 [00:01<00:03,  1.84trial/s, best loss: 0.6533530852831401]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-19 14:44:27,065 Previous dataset split restored\n",
      "/opt/atomsci-env/lib/python3.8/site-packages/xgboost/core.py:158: UserWarning: [14:44:27] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"n_gpus\", \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_performance|0.961|0.187|-1.070|1.610|-0.738|1.553|0.13276616347209194_1.5287856539810256_6_1.0_1.0_100_1.0|./dataset/final_models/final_data_curated_model_904acfb3-aa6f-4c44-8927-69996b7b5047.tar.gz\n",
      "\n",
      "xgb_gamma: 0.08310434725662126, xgb_learning_rate: 2.6550290489912314, xgb_max_depth: 6, xgb_colsample_bytree: 1.0, xgb_subsample: 1.0, xgb_n_estimators: 100, xgb_min_child_weight: 1.0\n",
      "xgboost model with computed_descriptors and rdkit_raw                                                    \n",
      " 40%|██████████████                     | 4/10 [00:02<00:03,  1.78trial/s, best loss: 0.6533530852831401]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-19 14:44:27,647 Previous dataset split restored\n",
      "/opt/atomsci-env/lib/python3.8/site-packages/xgboost/core.py:158: UserWarning: [14:44:27] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"n_gpus\", \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_performance|-7905541265163044280257994026761678542077952.000|2645674942034140463104.000|-5588543550836849974625723793965097859153920.000|2645675246345685303296.000|-5043482681164589389072055369118414352154624.000|2645675161714674368512.000|0.08310434725662126_2.6550290489912314_6_1.0_1.0_100_1.0|./dataset/final_models/final_data_curated_model_ada5cb3a-43d6-412f-92ef-54b3b7167d7c.tar.gz\n",
      "\n",
      "xgb_gamma: 0.06424743713427893, xgb_learning_rate: 0.5556871678753759, xgb_max_depth: 6, xgb_colsample_bytree: 1.0, xgb_subsample: 1.0, xgb_n_estimators: 100, xgb_min_child_weight: 1.0\n",
      "xgboost model with computed_descriptors and rdkit_raw                                                    \n",
      " 50%|█████████████████▌                 | 5/10 [00:02<00:02,  1.76trial/s, best loss: 0.6533530852831401]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-19 14:44:28,224 Previous dataset split restored\n",
      "/opt/atomsci-env/lib/python3.8/site-packages/xgboost/core.py:158: UserWarning: [14:44:28] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"n_gpus\", \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_performance|0.944|0.222|0.256|0.965|0.244|1.024|0.06424743713427893_0.5556871678753759_6_1.0_1.0_100_1.0|./dataset/final_models/final_data_curated_model_6b126e9f-1524-4641-b158-97f0183ad146.tar.gz\n",
      "\n",
      "xgb_gamma: 0.13039378161862977, xgb_learning_rate: 0.1813386969649306, xgb_max_depth: 6, xgb_colsample_bytree: 1.0, xgb_subsample: 1.0, xgb_n_estimators: 100, xgb_min_child_weight: 1.0\n",
      "xgboost model with computed_descriptors and rdkit_raw                                                    \n",
      " 60%|█████████████████████              | 6/10 [00:03<00:02,  1.75trial/s, best loss: 0.6533530852831401]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-19 14:44:28,829 Previous dataset split restored\n",
      "/opt/atomsci-env/lib/python3.8/site-packages/xgboost/core.py:158: UserWarning: [14:44:28] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"n_gpus\", \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_performance|0.847|0.368|0.331|0.916|0.289|0.993|0.13039378161862977_0.1813386969649306_6_1.0_1.0_100_1.0|./dataset/final_models/final_data_curated_model_99a29d26-a775-4cbc-92f8-e79a1c64a11f.tar.gz\n",
      "\n",
      "xgb_gamma: 0.05848213310584516, xgb_learning_rate: 0.793144063576144, xgb_max_depth: 6, xgb_colsample_bytree: 1.0, xgb_subsample: 1.0, xgb_n_estimators: 100, xgb_min_child_weight: 1.0\n",
      "xgboost model with computed_descriptors and rdkit_raw                                                    \n",
      " 70%|████████████████████████▌          | 7/10 [00:04<00:01,  1.73trial/s, best loss: 0.6533530852831401]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-19 14:44:29,427 Previous dataset split restored\n",
      "/opt/atomsci-env/lib/python3.8/site-packages/xgboost/core.py:158: UserWarning: [14:44:29] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"n_gpus\", \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_performance|0.951|0.209|0.129|1.044|0.099|1.118|0.05848213310584516_0.793144063576144_6_1.0_1.0_100_1.0|./dataset/final_models/final_data_curated_model_6b8e3e51-a7ac-4006-be7f-8d1a80cb04e7.tar.gz\n",
      "\n",
      "xgb_gamma: 0.15683305856616978, xgb_learning_rate: 0.8853426792430782, xgb_max_depth: 6, xgb_colsample_bytree: 1.0, xgb_subsample: 1.0, xgb_n_estimators: 100, xgb_min_child_weight: 1.0\n",
      "xgboost model with computed_descriptors and rdkit_raw                                                    \n",
      " 80%|████████████████████████████       | 8/10 [00:04<00:01,  1.73trial/s, best loss: 0.6533530852831401]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-19 14:44:29,991 Previous dataset split restored\n",
      "/opt/atomsci-env/lib/python3.8/site-packages/xgboost/core.py:158: UserWarning: [14:44:30] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"n_gpus\", \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_performance|0.925|0.257|0.064|1.083|-0.020|1.190|0.15683305856616978_0.8853426792430782_6_1.0_1.0_100_1.0|./dataset/final_models/final_data_curated_model_1c597f0b-b8f7-40e1-9ba7-c6041d54108a.tar.gz\n",
      "\n",
      "xgb_gamma: 0.04009766682360752, xgb_learning_rate: 0.387820507539886, xgb_max_depth: 6, xgb_colsample_bytree: 1.0, xgb_subsample: 1.0, xgb_n_estimators: 100, xgb_min_child_weight: 1.0\n",
      "xgboost model with computed_descriptors and rdkit_raw                                                    \n",
      " 90%|███████████████████████████████▌   | 9/10 [00:05<00:00,  1.76trial/s, best loss: 0.6533530852831401]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-19 14:44:30,535 Previous dataset split restored\n",
      "/opt/atomsci-env/lib/python3.8/site-packages/xgboost/core.py:158: UserWarning: [14:44:30] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"n_gpus\", \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_performance|0.925|0.258|0.315|0.926|0.306|0.981|0.04009766682360752_0.387820507539886_6_1.0_1.0_100_1.0|./dataset/final_models/final_data_curated_model_f4473d47-d9c6-456b-99ab-10cf319b5b98.tar.gz\n",
      "\n",
      "100%|██████████████████████████████████| 10/10 [00:05<00:00,  1.76trial/s, best loss: 0.6533530852831401]\n",
      "Generating the performance -- iteration table and Copy the best model tarball.\n",
      "Best model: ./dataset/final_models/final_data_curated_model_b6763e7f-d080-4cb1-863e-f9f177fc7433.tar.gz, valid R2: 0.34664691471685993\n"
     ]
    }
   ],
   "source": [
    "import atomsci.ddm.utils.hyperparam_search_wrapper as hsw\n",
    "import importlib\n",
    "importlib.reload(hsw)\n",
    "ampl_param = hsw.parse_params(params)\n",
    "hs = hsw.build_search(ampl_param)\n",
    "hs.run_search()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top scoring model will be saved in `dataset/SLC6A3_models/best_models` along with a csv file\n",
    "containing regression performance for all trained models.\n",
    "\n",
    "All of the models are saved in `dataset/SLC6A3_models`. These models can be\n",
    "explored using `get_filesystem_perf_results`. A full analysis of the hyperparameter performance is explored in **Tutorial 6, \"Compare models to select the best hyperparameters\"**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found data for 29 models under dataset/final_models\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_uuid</th>\n",
       "      <th>model_parameters_dict</th>\n",
       "      <th>best_valid_r2_score</th>\n",
       "      <th>best_test_r2_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>a0ac4513-a33a-4a39-a9f7-113d9b852864</td>\n",
       "      <td>{\"best_epoch\": 12, \"dropouts\": [0.342082063392...</td>\n",
       "      <td>0.403875</td>\n",
       "      <td>0.400500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>906072ff-83c1-4549-bf98-d473387e897e</td>\n",
       "      <td>{\"best_epoch\": 57, \"dropouts\": [0.016191935195...</td>\n",
       "      <td>0.398768</td>\n",
       "      <td>0.383271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>230e1138-fcac-4792-80aa-682657277c9f</td>\n",
       "      <td>{\"best_epoch\": 73, \"dropouts\": [0.113310441604...</td>\n",
       "      <td>0.398239</td>\n",
       "      <td>0.404253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>750b683a-07ce-4cd1-b0a7-c2c423f26e4c</td>\n",
       "      <td>{\"best_epoch\": 19, \"dropouts\": [0.033750916470...</td>\n",
       "      <td>0.378872</td>\n",
       "      <td>0.357292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>b6763e7f-d080-4cb1-863e-f9f177fc7433</td>\n",
       "      <td>{\"xgb_colsample_bytree\": 1.0, \"xgb_gamma\": 0.0...</td>\n",
       "      <td>0.346647</td>\n",
       "      <td>0.289184</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              model_uuid  \\\n",
       "19  a0ac4513-a33a-4a39-a9f7-113d9b852864   \n",
       "24  906072ff-83c1-4549-bf98-d473387e897e   \n",
       "26  230e1138-fcac-4792-80aa-682657277c9f   \n",
       "21  750b683a-07ce-4cd1-b0a7-c2c423f26e4c   \n",
       "15  b6763e7f-d080-4cb1-863e-f9f177fc7433   \n",
       "\n",
       "                                model_parameters_dict  best_valid_r2_score  \\\n",
       "19  {\"best_epoch\": 12, \"dropouts\": [0.342082063392...             0.403875   \n",
       "24  {\"best_epoch\": 57, \"dropouts\": [0.016191935195...             0.398768   \n",
       "26  {\"best_epoch\": 73, \"dropouts\": [0.113310441604...             0.398239   \n",
       "21  {\"best_epoch\": 19, \"dropouts\": [0.033750916470...             0.378872   \n",
       "15  {\"xgb_colsample_bytree\": 1.0, \"xgb_gamma\": 0.0...             0.346647   \n",
       "\n",
       "    best_test_r2_score  \n",
       "19            0.400500  \n",
       "24            0.383271  \n",
       "26            0.404253  \n",
       "21            0.357292  \n",
       "15            0.289184  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import atomsci.ddm.pipeline.compare_models as cm\n",
    "\n",
    "result_df = cm.get_filesystem_perf_results(\n",
    "    result_dir=model_dir,\n",
    "    pred_type='regression'\n",
    ")\n",
    "\n",
    "# sort by validation r2 score to see top performing models\n",
    "result_df = result_df.sort_values(by='best_valid_r2_score', ascending=False)\n",
    "result_df[['model_uuid','model_parameters_dict','best_valid_r2_score','best_test_r2_score']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv('./final_hypers.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples of Other Parameter Sets\n",
    "Below are some parameters that can be used for **neural networks**, \n",
    "**[XGBoost](https://en.wikipedia.org/wiki/XGBoost)** models, \n",
    "**fingerprint splits** and **[ECFP](https://pubs.acs.org/doi/10.1021/ci100050t)** features.\n",
    "Each set of parameters can be used to replace the parameters above. \n",
    "Trying them out is left as an exercise for the reader.\n",
    "\n",
    "#### Neural Network Hyperopt Search\n",
    "\n",
    "|Parameter|Description|\n",
    "|---|---|\n",
    "|`lr`| This controls the learning rate. loguniform\\|-13.8,-3 means the logarithm of the learning rate is uniformly distributed between -13.8 and -3.|\n",
    "|`ls` |This controls layer sizes. 3\\|8,512 means 3 layers with sizes ranging between 8 and 512 neurons. A good strategy is to start with a fewer layers and slowly increase the number until performance plateaus.| \n",
    "|`dp`| This controls dropout. 3\\|0,0.4 means 3 dropout layers with probability of zeroing a weight between 0 and 40%. This needs to match the number of layers specified with `ls` and should range between 0% and 50%. |\n",
    "|`max_epochs`| This controls how long to train each model. Training for more epochs increases runtime, but allows models more time to optimize. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"hyperparam\": \"True\",\n",
    "    \"prediction_type\": \"regression\",\n",
    "\n",
    "    \"dataset_key\": dataset_key,\n",
    "    \"id_col\": \"compound_id\",\n",
    "    \"smiles_col\": \"base_rdkit_smiles\",\n",
    "    \"response_cols\": \"Y\",\n",
    "\n",
    "    \"splitter\":\"scaffold\",\n",
    "    \"split_uuid\": split_uuid,\n",
    "    \"previously_split\": \"True\",\n",
    "\n",
    "    \"featurizer\": \"computed_descriptors\",\n",
    "    \"descriptor_type\" : descriptor_type,\n",
    "    \"transformers\": \"True\",\n",
    "\n",
    "    ### Use a NN model\n",
    "    \"search_type\": \"hyperopt\",\n",
    "    \"model_type\": \"NN|10\",\n",
    "    \"lr\": \"loguniform|-13.8,-3\",\n",
    "    \"ls\": \"uniformint|3|8,512\",\n",
    "    \"dp\": \"uniform|3|0,0.4\",\n",
    "    \"max_epochs\":100,\n",
    "    ###\n",
    "\n",
    "    \"result_dir\": f\"./{best_model_dir},./{model_dir}\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost\n",
    "- `xgbg` Stands for `xgb_gamma` and controls the minimum loss \n",
    "reduction required to make a further partition on a leaf node of the tree.\n",
    "- `xgbl` Stands for `xgb_learning_rate` and controls the boosting \n",
    "learning rate searching domain of  **[XGBoost](https://en.wikipedia.org/wiki/XGBoost)** models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"hyperparam\": \"True\",\n",
    "    \"prediction_type\": \"regression\",\n",
    "\n",
    "    \"dataset_key\": dataset_key,\n",
    "    \"id_col\": \"compound_id\",\n",
    "    \"smiles_col\": \"base_rdkit_smiles\",\n",
    "    \"response_cols\": \"Y\",\n",
    "\n",
    "    \"splitter\":\"scaffold\",\n",
    "    \"split_uuid\": split_uuid,\n",
    "    \"previously_split\": \"True\",\n",
    "\n",
    "    \"featurizer\": \"computed_descriptors\",\n",
    "    \"descriptor_type\" : descriptor_type,\n",
    "    \"transformers\": \"True\",\n",
    "\n",
    "    ### Use an XGBoost model\n",
    "    \"search_type\": \"hyperopt\",\n",
    "    \"model_type\": \"xgboost|10\",\n",
    "    \"xgbg\": \"uniform|0,0.2\",\n",
    "    \"xgbl\": \"loguniform|-2,2\",\n",
    "    ###\n",
    "\n",
    "    \"result_dir\": f\"./{best_model_dir},./{model_dir}\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fingerprint Split\n",
    "This trains an  **[XGBoost](https://en.wikipedia.org/wiki/XGBoost)** model using a\n",
    "**fingerprint split**. The fingerprint split is provided with the dataset files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_split_uuid=\"be60c264-6ac0-4841-a6b6-41bf846e4ae4\"\n",
    "\n",
    "params = {\n",
    "    \"hyperparam\": \"True\",\n",
    "    \"prediction_type\": \"regression\",\n",
    "\n",
    "    \"dataset_key\": dataset_key,\n",
    "    \"id_col\": \"compound_id\",\n",
    "    \"smiles_col\": \"base_rdkit_smiles\",\n",
    "    \"response_cols\": \"avg_pKi\",\n",
    "\n",
    "    ### Use a fingerprint split\n",
    "    \"splitter\":\"fingerprint\",\n",
    "    \"split_uuid\": fp_split_uuid,\n",
    "    \"previously_split\": \"True\",\n",
    "    ###\n",
    "\n",
    "    \"featurizer\": \"computed_descriptors\",\n",
    "    \"descriptor_type\" : descriptor_type,\n",
    "    \"transformers\": \"True\",\n",
    "\n",
    "    \"search_type\": \"hyperopt\",\n",
    "    \"model_type\": \"xgboost|10\",\n",
    "    \"xgbg\": \"uniform|0,0.2\",\n",
    "    \"xgbl\": \"loguniform|-2,2\",\n",
    "\n",
    "    \"result_dir\": f\"./{best_model_dir},./{model_dir}\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ECFP Features\n",
    "This uses an  **[XGBoost](https://en.wikipedia.org/wiki/XGBoost)** model with **[ECFP fingerprints](https://pubs.acs.org/doi/10.1021/ci100050t)** features and a **scaffold split**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"hyperparam\": \"True\",\n",
    "    \"prediction_type\": \"regression\",\n",
    "\n",
    "    \"dataset_key\": dataset_key,\n",
    "    \"id_col\": \"compound_id\",\n",
    "    \"smiles_col\": \"base_rdkit_smiles\",\n",
    "    \"response_cols\": \"avg_pKi\",\n",
    "\n",
    "    \"splitter\":\"scaffold\",\n",
    "    \"split_uuid\": split_uuid,\n",
    "    \"previously_split\": \"True\",\n",
    "\n",
    "    ### Use ECFP Features\n",
    "    \"featurizer\": \"ecfp\",\n",
    "    \"ecfp_radius\" : 2,\n",
    "    \"ecfp_size\" : 1024,\n",
    "    \"transformers\": \"True\",\n",
    "    ###\n",
    "\n",
    "    \"search_type\": \"hyperopt\",\n",
    "    \"model_type\": \"xgboost|10\",\n",
    "    \"xgbg\": \"uniform|0,0.2\",\n",
    "    \"xgbl\": \"loguniform|-2,2\",\n",
    "\n",
    "    \"result_dir\": f\"./{best_model_dir},./{model_dir}\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In **Tutorial 6, \"Compare Models to Select the Best Hyperparameters\"**, we analyze the performance of these large sets of models to select the best hyperparameters for production models.\n",
    "\n",
    "If you have specific feedback about a tutorial, please complete the **[AMPL Tutorial Evaluation](https://forms.gle/pa9sHj4MHbS5zG7A6)**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atomsci-env",
   "language": "python",
   "name": "atomsci-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
